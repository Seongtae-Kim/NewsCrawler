{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.firefox.firefox_binary import FirefoxBinary\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium import webdriver\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from datetime import datetime, date\n",
    "from lxml import etree as ET\n",
    "from lxml.builder import E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contemporary Chosun Crawler\n",
    "조선일보 2012, 2013, 2015, 2016, 2017년도 기사를 크롤링합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2012 = date.fromisoformat('2012-01-12')\n",
    "y_2013 = date.fromisoformat('2013-01-18')\n",
    "y_2015 = date.fromisoformat('2015-01-01')\n",
    "y_2016 = date.fromisoformat('2016-01-01')\n",
    "y_2017 = date.fromisoformat('2017-01-01')\n",
    "datelist2012 = pd.date_range(y_2012, periods=355) # from 2012-01-12 to 2012-12-31\n",
    "datelist2013 = pd.date_range(y_2013, periods=348) # from 2013-01-01 to 2013-12-31\n",
    "datelist2015 = pd.date_range(y_2015, periods=365) # from 2015-01-01 to 2015-12-31\n",
    "datelist2016 = pd.date_range(y_2016, periods=366) # from 2016-01-01 to 2016-12-31 (leap year)\n",
    "datelist2017 = pd.date_range(y_2017, periods=365) # from 2017-01-01 to 2017-12-31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "349\n"
     ]
    }
   ],
   "source": [
    "datelist = [] # 2013\n",
    "for date in datelist2013:\n",
    "    d = str(date).split(\" \")[0]\n",
    "    d = d.replace(\"-\", \"\")\n",
    "    datelist.append(d)\n",
    "print(len(datelist)) # 1827 days to crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_sample = \"http://news.chosun.com/svc/list_in/list.html?indate=\"\n",
    "\n",
    "urls=[]\n",
    "\n",
    "for date in datelist:\n",
    "    urls.append(url_sample+date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Article:\n",
    "    title=\"\"\n",
    "    category=\"\"\n",
    "    paragraph=\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_writer(article, date):\n",
    "    name = date + \"_\" + article.category + \"_\" + article.title\n",
    "    txt = \"\"\n",
    "    txt = txt + article.paragraph\n",
    "    name = name.replace(\"/\", \" \")\n",
    "    \n",
    "    with open(\"./output/\"+ name +\".txt\", \"w\") as text_file:\n",
    "        text_file.write(txt)\n",
    "\n",
    "def xml_writer(article, date):\n",
    "    name = date + \"_\" + article.category + \"_\" + article.title\n",
    "    name = name.replace(\"/\", \" \")\n",
    "    title = E(\"title\", article.title)\n",
    "    category = E(\"category\", article.category)\n",
    "    paragraph = E(\"paragraph\", article.paragraph)\n",
    "    article_xml = E(\"article\", category, title, paragraph)\n",
    "    \n",
    "    declaration = \"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
    "    <!DOCTYPE articles SYSTEM \"articles.dtd\">\n",
    "    \"\"\"\n",
    "    dec_byte = str.encode(declaration)\n",
    "    with open(\"./output/\"+ name +\".xml\", \"wb\") as f:\n",
    "        f.write(dec_byte)\n",
    "        f.write(ET.tostring(article_xml, xml_declaration=False, encoding=\"utf-8\", pretty_print=True))\n",
    "        \n",
    "def article_extractor(article_urls, article_list, date):\n",
    "    for article in article_urls:\n",
    "        a = Article()\n",
    "        browser.get(article)\n",
    "        title_exist = len(browser.find_elements_by_id(\"news_title_text_id\"))\n",
    "        cat_exist = len(browser.find_elements_by_id(\"news_cat_trig_id\"))\n",
    "        par_exist = len(browser.find_elements_by_class_name(\"par\"))\n",
    "        if title_exist != 0 and cat_exist != 0 and par_exist != 0:\n",
    "            a.title = browser.find_elements_by_id(\"news_title_text_id\")[0].text\n",
    "            a.category = browser.find_elements_by_id(\"news_cat_trig_id\")[0].text\n",
    "            a.paragraph = browser.find_elements_by_class_name(\"par\")[0].text\n",
    "            text_writer(a, date)\n",
    "            xml_writer(a, date)\n",
    "\n",
    "            #article_list.append(a)\n",
    "            print(date, len(article_list),\"번째 기사: '\", a.title, \"'완료\")\n",
    "\n",
    "def list_extractor(page_url, article_list, date):\n",
    "    articles = browser.find_elements_by_class_name(\"list_item\")\n",
    "    article_urls=[]\n",
    "    for article in articles:\n",
    "        if len(article.find_elements_by_class_name(\"author\")) != 0:\n",
    "            author = article.find_elements_by_class_name(\"author\")[0]\n",
    "            if len(author.text) >= 5:\n",
    "                if author.text[:5] == \"스포츠조선\": # Excluding Sports Articles\n",
    "                    pass\n",
    "                else:\n",
    "                    article_urls.append(article.find_elements_by_tag_name(\"a\")[0].get_attribute(\"href\"))\n",
    "            else:\n",
    "                if len(author.text) >= 3:\n",
    "                    if author.text[:5] == \"뉴시스\": # Excluding Newsis Articles\n",
    "                        pass\n",
    "                    else:\n",
    "                        article_urls.append(article.find_elements_by_tag_name(\"a\")[0].get_attribute(\"href\"))\n",
    "                else:\n",
    "                    article_urls.append(article.find_elements_by_tag_name(\"a\")[0].get_attribute(\"href\"))\n",
    "    article_extractor(article_urls, article_list, date)\n",
    "\n",
    "def page_browser(url, article_list): # browsing pages of that date\n",
    "    i=1 # page index\n",
    "    while len(browser.find_elements_by_class_name(\"list_item\")) >= 1:\n",
    "        if i == 1:\n",
    "            page = \"&pn=\" + str(i)\n",
    "            date = url[-8:]\n",
    "            page_url = url + page\n",
    "            browser.get(page_url)\n",
    "        list_extractor(page_url, article_list, date)\n",
    "        i += 1\n",
    "        page = \"&pn=\" + str(i)\n",
    "        page_url = url + page\n",
    "        browser.get(page_url)\n",
    "        \n",
    "def contemporary_chosun_crawler(urls):\n",
    "    article_list=[]\n",
    "    for url in urls:\n",
    "        browser.get(url)\n",
    "        page_browser(url, article_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20130117 0 번째 기사: ' [아침논단] 中企·서비스업 육성, 中·日 시장 겨냥해야 '완료\n",
      "20130117 0 번째 기사: ' [윤희영의 News English] 기이한 법들 : Weird laws '완료\n",
      "20130117 0 번째 기사: ' [태평로] '미래'가 '과거'에게 밀린 진보 좌파 '완료\n",
      "20130117 0 번째 기사: ' [허동현의 모던 타임스] [40] \"화사한 쇼윈도의 舶來品에 얼빠진 몽유병자들이여\" '완료\n",
      "20130117 0 번째 기사: ' [아침 편지] 외국인 며느리들의 눈물이 꽃으로 피어나길 '완료\n",
      "20130117 0 번째 기사: ' [만물상] 편의점 최저임금 '완료\n",
      "20130117 0 번째 기사: ' [조선데스크] 세대를 화합시키는 음악 '완료\n",
      "20130117 0 번째 기사: ' [가슴으로 읽는 동시] 어머니 '완료\n",
      "20130117 0 번째 기사: ' \"KTX만큼 싸게!\"…비행기의 몸부림 '완료\n",
      "20130117 0 번째 기사: ' 美 시티그룹, 작년 4분기 실적 예상보다 부진 '완료\n",
      "20130117 0 번째 기사: ' 美 실업수당 건수 33만5000건…5년만에 가장 적어 '완료\n",
      "20130117 0 번째 기사: ' 美 12월 주택착공 12.1% 증가…전문가 예상 웃돌아 '완료\n",
      "20130117 0 번째 기사: ' BoA, 작년 4분기 순익 급감…예상보다는 선전 '완료\n",
      "20130117 0 번째 기사: ' [TV조선]한달전 '안전'판정 받았는데…영화보다 천장 '와르르' '완료\n",
      "20130117 0 번째 기사: ' 유도 김재범, 오는 3월 2세 연하 예비신부와 결혼 '완료\n",
      "20130117 0 번째 기사: ' 이정희 \"서로에게 대선 패배 책임 떠넘기지 말자\" '완료\n",
      "20130117 0 번째 기사: ' 朴당선인 취임식, 일반국민 초청 늘 듯 '완료\n",
      "20130117 0 번째 기사: ' '록한류' 검엑스, 3년만에 컴백 공연 '완료\n",
      "20130117 0 번째 기사: ' 민주당 \"감사원, 1차는 눈치보기·2차는 늑장 발표\" '완료\n",
      "20130117 0 번째 기사: ' 이하이 골든디스크 음원 부문 신인상 \"삼촌 팬 덕\" '완료\n",
      "20130117 0 번째 기사: ' \"승기 왜 이래?\"...광고 호감도 싸이 5위·이승기 1위 '완료\n",
      "20130117 0 번째 기사: ' '동성결혼' 엘튼 존 부부, 대리모 통해 둘째 아들 얻어 '완료\n",
      "20130117 0 번째 기사: ' 션-정혜영, 취약계층 아동 교육기금으로 5년째 5억 기부 '완료\n",
      "20130117 0 번째 기사: ' 감사원 \"4대강 사업 보 설계부실… 음용수 안전성도 위협해\" '완료\n",
      "20130117 0 번째 기사: ' \"이동흡, 여직원에게 법복 입히고 벗기도록 해\" '완료\n",
      "20130117 0 번째 기사: ' 인수위, '불통' 논란 이어 '北 해킹' 해프닝까지 '완료\n",
      "20130117 0 번째 기사: ' 샤프, 中 LCD TV 공장 레노버에 매각 추진 '완료\n",
      "20130117 0 번째 기사: ' 中企부·농축산식품부?…정부조직개편안, 국회서 수정되나 '완료\n",
      "20130117 0 번째 기사: ' [오늘의 운세] 1월 18일 금요일(음력 12월 7일 甲申) '완료\n",
      "20130117 0 번째 기사: ' 朴 복지공약, 실현가능성 논란…인수위, 공약수정 나서나 '완료\n",
      "20130117 0 번째 기사: ' \"S대 예비합격\" 네티즌 성적표에 관심 쏠려 '완료\n",
      "20130117 0 번째 기사: ' 배우 이보영, 12년전 '사랑의 스튜디오' 출연 모습 '완료\n",
      "20130117 0 번째 기사: ' [글로벌뷰] 中, 해양경제 활성화…주변국과 마찰 심화 '완료\n",
      "20130117 0 번째 기사: ' 7세 아이 앞 '바바리맨', 아동학대죄에 해당 '완료\n",
      "20130117 0 번째 기사: ' [영화리뷰] '로봇 앤 프랭크' '완료\n",
      "20130117 0 번째 기사: ' \"미국 20년후 세계 최대 에너지수출국 된다\" '완료\n",
      "20130117 0 번째 기사: ' 월가 보너스 철 맞아 보너스 랭킹 앱 등장 '완료\n",
      "20130117 0 번째 기사: ' [김명환의 씨네칵테일] 웃기다가 울리는 ‘한국형’ 코미디 ‘박수건달’의 한계 '완료\n",
      "20130117 0 번째 기사: ' 인수위, 정부 업무보고 마무리…22일까지 정책간담회 '완료\n",
      "20130117 0 번째 기사: ' 단국대 \"김재우 방문진이사장 박사학위 논문은 표절\" '완료\n",
      "20130117 0 번째 기사: ' \"중남미 기업인들이 올해 경기 가장 낙관적\" '완료\n",
      "20130117 0 번째 기사: ' \"美·中 무역 불균형, 부풀려졌다\" '완료\n",
      "20130117 0 번째 기사: ' 이태임 '돈의 화신'에서 하차...최여진이 맡기로 '완료\n",
      "20130117 0 번째 기사: ' 고비넘긴 中 대기오염…경제문제로 이슈화 '완료\n",
      "20130117 0 번째 기사: ' `특수강도' 前국가대표 축구선수 김동현 법정구속 '완료\n",
      "20130117 0 번째 기사: ' 인수위-새누리당, 불협화음 '표면화'…철통보안 '불통' 논란 '완료\n",
      "20130117 0 번째 기사: ' 이별 직후, 男 \"술마신다\" 女 \"페이스북 사진 바꾼다\" '완료\n",
      "20130117 0 번째 기사: ' 인수위 \"北 기자실 해킹 시도는 일부 오해…해킹 여부는 국가보안 사항이라 구체적으로 말할 수 없어\" '완료\n",
      "20130117 0 번째 기사: ' 美 댈러스 연은 총재, 양적완화 회의론 '완료\n",
      "20130117 0 번째 기사: ' 미 총기규제 발표에 총기관련주 급등 '완료\n",
      "20130117 0 번째 기사: ' 인수위, \"北 해킹 확인된바 없다\" '완료\n",
      "20130117 0 번째 기사: ' 시아버지의 용서에 시어머니 살해한 며느리 항소심서 감형 '완료\n",
      "20130117 0 번째 기사: ' 외국자금 돌아온 伊, 30년물 초장기 국채 재발행 임박 '완료\n",
      "20130117 0 번째 기사: ' 슈퍼걸즈 카노 카에데, 임신 중절 폭로 끝 팀 탈퇴 '완료\n",
      "20130117 0 번째 기사: ' [TV조선]'김국'과 '미자탕'…남도의 끝, 해남의 맛을 찾아서 '완료\n",
      "20130117 0 번째 기사: ' 월가 거액 보너스 관행 바뀐다 '완료\n",
      "20130117 0 번째 기사: ' 윤지영 아나운서 집공개 \"거실이야~서재야~\" '완료\n",
      "20130117 0 번째 기사: ' 소송 패하자 산후조리원에서 공기총 쏘고 흉기난동 50대 검거 '완료\n",
      "20130117 0 번째 기사: ' 타이거 우즈, 前부인에 재결합 요구하며 2100억원 제시 '완료\n",
      "20130117 0 번째 기사: ' 해적소탕 작전 덕에 해적 선박 납치시도 건수 5년만에 최저 '완료\n",
      "20130117 0 번째 기사: ' '동성애' 엘튼 존, 대리모 통해 둘째 아들 얻어 '완료\n",
      "20130117 0 번째 기사: ' '립카페' 갔다 경찰 적발된 경남도의원, 눈물의 사퇴 '완료\n"
     ]
    }
   ],
   "source": [
    "browser = webdriver.Firefox()\n",
    "contemporary_chosun_crawler(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
